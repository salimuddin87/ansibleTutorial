# Copyright 2018 bad_db. All rights reserved.
# tddb CONFIDENTIAL AND TRADE SECRET

apiVersion: v1
kind: List
items:
  - apiVersion: v1
    kind: ConfigMap
    metadata:
      name: elasticsearch-logging-v1
      namespace: kube-system
      labels:
        app: elasticsearch-logging
        version: v1
    data:
      logstash.conf: |-
            input {
                beats {
                    port => 5000
              }
            }

            ## Add your filters / logstash plugins configuration here

            filter {
              if [type] == "kube-logs" {
                json {
                  source => "message"
                }

                date {
                  match => ["time", "ISO8601"]
                }

                mutate {
                    rename => ["log" , "message"]
                    rename => ["time" , "event_time"]
                    remove_field => ["offset", "input_type", "count", "type", "beat", "stream"]
                }

                grok {
                    match => {
                      "source" => "/var/log/containers/%{DATA:pod_name}_%{DATA:namespace}_%{GREEDYDATA:resource_name}-%{DATA:container_id}.log"
                    }
                    remove_field => ["source"]
                }

                ## Kubernetes state nested into a subjson
                mutate {
                    rename => {
                        "[fields][host]" => "node"
                        "resource_name" => "[kubernetes][resource_name]"
                        "pod_name" => "[kubernetes][pod_name]"
                        "container_id" => "[kubernetes][container_id]"
                        "namespace" => "[kubernetes][namespace]"
                    }
                    remove_field => ["fields"]
                }

                grok {
                    match => {
                      "logline" => "\s+%{LOGLEVEL:level}\s+"
                    }
                }
              }
            }

            output {
                if "service" in [component]{
                    elasticsearch {
                        ##containers within the same pod, talk over 127.0.0.1
                        hosts => "127.0.0.1:9200"
                        index => "service-logs"
                        template_name => "service-logs"
                        template => "/etc/logstash/conf.d/elasticsearch/service-logs.json"
                        template_overwrite => true
                    }
                }else if [component] =~ /\d+/ {
                    elasticsearch {
                        hosts => "127.0.0.1:9200"
                        index => "app-logs"
                        template_name => "app-logs"
                        template => "/etc/logstash/conf.d/elasticsearch/app-logs.json"
                        template_overwrite => true
                    }
                }else{
                    elasticsearch {
                        hosts => "127.0.0.1:9200"
                    }
                }
            }
      logstash.service.logs.json: |-
            {"template":"service-*","mappings":{"_default_":{"dynamic_templates":[{"logline_field":{"mapping":{"index":"not_analyzed","type":"string"},"match":"logline"}},{"component_field":{"mapping":{"index":"not_analyzed","type":"string"},"match":"component"}}]}}}

      logstash.app.logs.json: |-
            {"template":"app-*","mappings":{"_default_":{"dynamic_templates":[{"logline_field":{"mapping":{"index":"not_analyzed","type":"string"},"match":"logline"}},{"component_field":{"mapping":{"index":"not_analyzed","type":"string"},"match":"component"}}]}}}
  - apiVersion: apps/v1beta1
    kind: Deployment
    metadata:
      name: elasticsearch-logging-v5.6.2
      namespace: kube-system
      labels:
        role: frontend
        app: elasticsearch-logging
        version: v5.6.2
        kubernetes.io/cluster-service: "true"
    spec:
      replicas: {{ master_count.stdout }}
      selector:
        matchLabels:
          app: elasticsearch-logging
      template:
        metadata:
          labels:
            role: frontend
            app: elasticsearch-logging
            version: v5.6.2
            kubernetes.io/cluster-service: "true"
          annotations:
            scheduler.alpha.kubernetes.io/critical-pod: ''
        spec:
          nodeSelector:
            node-role.kubernetes.io/master: ''
          tolerations:
            # Mark the pod as a critical add-on for rescheduling.
            - key: "CriticalAddonsOnly"
              operator: "Exists"
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
              - labelSelector:
                  matchExpressions:
                  - key: component
                    operator: In
                    values:
                    - elasticsearch-logging
                topologyKey: kubernetes.io/hostname
          containers:
          - image: gcr.io/google_containers/elasticsearch:v5.6.2
            name: elasticsearch-logging
            env:
            - name: "ES_JAVA_OPTS"
              value: {{ ELASTICSEARCH_MEM_SIZE }}
            resources:
              # keep request = limit to keep this container in guaranteed class
              limits:
                cpu: {{ ELASTICSEARCH_CPU_SIZE }}
              requests:
                cpu: 100m
            ports:
            - containerPort: 9200
              name: db
              protocol: TCP
            - containerPort: 9300
              name: transport
              protocol: TCP
            volumeMounts:
            - name: elastic-storage
              mountPath: /data
          - image: registry.uda.io/k8s/logstash:v5.4.1
            name: logstash
            env:
            - name: "LS_JAVA_OPTS"
              value: {{ LOGSTASH_MEM_SIZE }}
            ports:
            - containerPort: 5000
              name: logstash
              protocol: TCP
            volumeMounts:
            - mountPath: /etc/logstash/conf.d
              name: logstash-config
          - image: crobox/elasticsearch-exporter
            name: elasticsearch-exporter
            env:
            - name: ELASTICSEARCH_ADDRESS
              value: "elasticsearch-logging.kube-system.svc.cluster.local:9200"
            ports:
            - containerPort: 9108
              name: exporter
              protocol: TCP
          volumes:
          - name: elastic-storage
            emptyDir: {}
          - name: logstash-config
            configMap:
              name: elasticsearch-logging-v1
              items:
              - key: logstash.conf
                path: config/logstash.conf
              - key: logstash.service.logs.json
                path: elasticsearch/service-logs.json
              - key: logstash.app.logs.json
                path: elasticsearch/app-logs.json
          # Elasticsearch requires vm.max_map_count to be at least 262144.
          # If your OS already sets up this number to a higher value, feel free
          # to remove this init container.
          initContainers:
          - image: alpine:3.6
            command: ["/sbin/sysctl", "-w", "vm.max_map_count=262144"]
            name: elasticsearch-logging-init
            securityContext:
              privileged: true
  - apiVersion: v1
    kind: Service
    metadata:
      name: elasticsearch-logging
      namespace: kube-system
      labels:
        app: elasticsearch-logging
        kubernetes.io/cluster-service: "true"
        kubernetes.io/name: "Elasticsearch"
    spec:
      ports:
      - port: 9200
        name: db
        protocol: TCP
        targetPort: db
      - port: 9300
        name: transport 
        protocol: TCP
        targetPort: transport 
      - port: 5000 
        name: logstash
        protocol: TCP
        targetPort: logstash 
      selector:
        app: elasticsearch-logging
